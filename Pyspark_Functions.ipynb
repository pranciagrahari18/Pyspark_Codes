{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89249737-30ba-4f3a-8c74-0b1e96ad4459",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b983d4d-8c38-4e3a-8d10-37310cd491e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  Name|Salary|\n+---+------+------+\n|  1|Pranci|  2000|\n|  2| Tanna|  5000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data = [(1,\"Pranci\",2000),(2,\"Tanna\",5000)]\n",
    "df = spark.createDataFrame(data = data, schema = [\"id\",\"Name\",\"Salary\"])\n",
    "df1 = df.withColumn(colName=\"Salary\",col = col(\"Salary\").cast(\"Integer\")) #cast is used for change the datatype of a column\n",
    "df1.show()\n",
    "#withColumn() is used to create a new column or changeing the data into existing column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c974ceb-c8ce-4de1-a776-34348fa8f666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  Name|Salary|\n+---+------+------+\n|  1|Pranci|  4000|\n|  2| Tanna| 10000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"Salary\",col(\"Salary\")*2) # Col is used to change the data of a column\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185c218b-d618-4b82-add8-65fd70db8892",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n| id|  Name|Salary|Country|\n+---+------+------+-------+\n|  1|Pranci|  4000|  India|\n|  2| Tanna| 10000|  India|\n+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df3 = df2.withColumn(\"Country\",lit(\"India\")) # lit is used to add new column with same data in the column\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28e9ef75-063c-488d-b192-741cb885d6ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#withColumnRenamed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c90032-4bb6-4ee2-9ea2-50a9e94fb592",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n| id|  Name|Salary_amount|\n+---+------+-------------+\n|  1|Pranci|         2000|\n|  2| Tanna|         5000|\n+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,\"Pranci\",2000),(2,\"Tanna\",5000)]\n",
    "df = spark.createDataFrame(data = data, schema = [\"id\",\"Name\",\"Salary\"])\n",
    "df1 = df.withColumnRenamed(\"Salary\",\"Salary_amount\")\n",
    "df1.show()\n",
    "# withColumnRenamed() is used to change existing column name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef0426df-a530-4feb-a3e8-f0f602c1b08b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#StructType() and StructField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e016b4f9-bc7c-4768-91e9-c57c38573464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  1|Pranci|\n|  2| Tanna|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import*\n",
    "data = [(1,\"Pranci\"),(2,\"Tanna\")]\n",
    "schema = StructType([StructField(name = 'id',dataType = IntegerType()),\n",
    "                     StructField(name = 'name',dataType = StringType())])\n",
    "df = spark.createDataFrame(data = data, schema = ['id','name'])\n",
    "df.show()\n",
    "#It is used to programmitically specify the schema to the Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0888c28e-ecac-48e5-9748-9c327de6078e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#ArrayType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c6b027-f798-46e0-948c-18f7d1d1095d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n|     id|number|\n+-------+------+\n| Pranci|[1, 2]|\n|  Tanna|[4, 5]|\n|Aakrati|[7, 8]|\n+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "data = [('Pranci',[1,2]),('Tanna',[4,5]),('Aakrati',[7,8])]\n",
    "schema = StructType([StructField('id',StringType()),\n",
    "                    StructField('number',ArrayType(IntegerType()))])\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "#this function is used to deal with array type data in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2349e2c9-e597-4df4-a1cc-d38ef19a254b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a15d8c6-d858-4eb2-9bfc-913205aae526",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------+-------+\n| id|  Name|            Skill| Skills|\n+---+------+-----------------+-------+\n|  1|Pranci| [Pyspark, Azure]|Pyspark|\n|  1|Pranci| [Pyspark, Azure]|  Azure|\n|  2| Tanna|[BigData, Python]|BigData|\n|  2| Tanna|[BigData, Python]| Python|\n+---+------+-----------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,\"Pranci\",[\"Pyspark\",\"Azure\"]),(2,\"Tanna\",[\"BigData\",\"Python\"])]\n",
    "df = spark.createDataFrame(data = data, schema = [\"id\",\"Name\",\"Skill\"])\n",
    "from pyspark.sql.functions import explode,col\n",
    "df1 = df.withColumn(\"Skills\",explode(col(\"Skill\")))\n",
    "df1.show()\n",
    "# This function is used to create a new row for each element in the given array column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6bd622b-67f9-4e45-bd79-a1a2710e042f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa2a4cf5-892d-487e-8b92-0e1bdcef2872",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------+---------------+----------------+\n| id|  name|         skill|Primary_Skillls|Secondary_Skills|\n+---+------+--------------+---------------+----------------+\n|  1|Pranci| Pyspark,Azure|        Pyspark|           Azure|\n|  2| Tanna|BigData,Python|        BigData|          Python|\n+---+------+--------------+---------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "data = [(1,'Pranci',\"Pyspark,Azure\"),(2,'Tanna',\"BigData,Python\")]\n",
    "schema = [\"id\",\"name\",\"skill\"]\n",
    "df = spark.createDataFrame(data,schema)\n",
    "from pyspark.sql.functions import split,col\n",
    "df1 = df.withColumn(\"Primary_Skillls\",split(col(\"skill\"),\",\").getItem(0))\n",
    "df2 = df1.withColumn(\"Secondary_Skills\",split(col(\"skill\"),\",\").getItem(1))\n",
    "df2.show()\n",
    "#This function return an array type after splitting the string column by delimiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64f23610-371b-403d-95c9-3611d205ca75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d76259-d742-4643-b36d-401d24d2b771",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+--------------+-----------------+\n| id|  name|primaryskill|secondaryskill|       skillArray|\n+---+------+------------+--------------+-----------------+\n|  1|Pranci|     Pyspark|         azure| [Pyspark, azure]|\n|  2| Tanna|     BigData|        Python|[BigData, Python]|\n+---+------+------------+--------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,\"Pranci\",\"Pyspark\",\"azure\"),(2,\"Tanna\",\"BigData\",\"Python\")]\n",
    "schema = [\"id\",\"name\",\"primaryskill\",\"secondaryskill\"]\n",
    "df = spark.createDataFrame(data ,schema)\n",
    "from pyspark.sql.functions import array,col\n",
    "df1 = df.withColumn(\"skillArray\",array(col(\"primaryskill\"),col(\"secondaryskill\")))\n",
    "df1.show()\n",
    "# This function is used to create a new column by merging the data from multiple column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35d0b6ab-c6fb-4d64-b8a4-795b2c7debcc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#array_contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a73e1fe-0a55-4318-9f11-4f3912e3689d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------+------------+\n| id|  name|            skill|HasJavaSkill|\n+---+------+-----------------+------------+\n|  1|Pranci| [Pyspark, azure]|       false|\n|  2| Tanna|[BigData, Python]|       false|\n+---+------+-----------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,\"Pranci\",[\"Pyspark\",\"azure\"]),(2,\"Tanna\",[\"BigData\",\"Python\"])]\n",
    "schema = [\"id\",\"name\",\"skill\"]\n",
    "df = spark.createDataFrame(data ,schema)\n",
    "from pyspark.sql.functions import array_contains\n",
    "df1 = df.withColumn(\"HasJavaSkill\",array_contains(col(\"skill\"),\"java\"))\n",
    "df1.show()\n",
    "#This function is used to check whether the data is present or not if present then return True otherwise False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd9ada0-b27f-43fa-b02b-84779272cb24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#MapType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24727254-3c54-4c3e-9660-75c664a6e4bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n|  name|          properties|\n+------+--------------------+\n|Pranci|{eye -> brown, ha...|\n| Tanna|{eye -> blue, hai...|\n+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Method - 1\n",
    "data = [(\"Pranci\",{\"hair\":\"black\",\"eye\":\"brown\"}),(\"Tanna\",{\"hair\":\"black\",\"eye\":\"blue\"})]\n",
    "schema = [\"name\",\"properties\"]\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea846c8b-42f1-4853-a719-7098cdfa100e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+\n|name  |properties                   |\n+------+-----------------------------+\n|Pranci|{eye -> brown, hair -> black}|\n|Tanna |{eye -> blue, hair -> black} |\n+------+-----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Method - 2\n",
    "from pyspark.sql.types import StructField, StructType,StringType,MapType\n",
    "data = [(\"Pranci\",{\"hair\":\"black\",\"eye\":\"brown\"}),(\"Tanna\",{\"hair\":\"black\",\"eye\":\"blue\"})]\n",
    "schema = StructType([StructField(\"name\",StringType()),\n",
    "                     StructField(\"properties\",MapType(StringType(),StringType()))])\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f6cba6-411b-4f69-bc6d-df0178f7ce1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+-----+\n|  name|          properties| hair|  eye|\n+------+--------------------+-----+-----+\n|Pranci|{eye -> brown, ha...|black|brown|\n| Tanna|{eye -> blue, hai...|black| blue|\n+------+--------------------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Access MapType Elements\n",
    "df1 = df.withColumn(\"hair\",df.properties[\"hair\"])\n",
    "df2 = df1.withColumn(\"eye\",df.properties[\"eye\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afe787e5-9091-486d-aeae-2906aa6ab24d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Functions Work with MapColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2772012a-2375-47ff-98bb-d28df584e3a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+\n|name  |properties                   |\n+------+-----------------------------+\n|Pranci|{eye -> brown, hair -> black}|\n|Tanna |{eye -> blue, hair -> black} |\n+------+-----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType,StringType,MapType\n",
    "data = [(\"Pranci\",{\"hair\":\"black\",\"eye\":\"brown\"}),(\"Tanna\",{\"hair\":\"black\",\"eye\":\"blue\"})]\n",
    "schema = StructType([StructField(\"name\",StringType()),\n",
    "                     StructField(\"properties\",MapType(StringType(),StringType()))])\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c700b2-ca42-437e-8e3b-e2260d931d84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+----+-----+\n|name  |properties                   |key |value|\n+------+-----------------------------+----+-----+\n|Pranci|{eye -> brown, hair -> black}|eye |brown|\n|Pranci|{eye -> brown, hair -> black}|hair|black|\n|Tanna |{eye -> blue, hair -> black} |eye |blue |\n|Tanna |{eye -> blue, hair -> black} |hair|black|\n+------+-----------------------------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Function - 1 --> explode()\n",
    "from pyspark.sql.functions import explode\n",
    "df1 = df.select(\"name\",\"properties\",explode(df.properties))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a00a4fa-7d03-471f-b139-cae155016742",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+-----------+\n|name  |properties                   |keys       |\n+------+-----------------------------+-----------+\n|Pranci|{eye -> brown, hair -> black}|[eye, hair]|\n|Tanna |{eye -> blue, hair -> black} |[eye, hair]|\n+------+-----------------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Function - 2 --> map_keys()\n",
    "from pyspark.sql.functions import map_keys\n",
    "df1 = df.withColumn(\"keys\",map_keys(df.properties))\n",
    "df1.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c43a93-2d0f-4861-8196-4cb98b9414c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+--------------+\n|name  |properties                   |values        |\n+------+-----------------------------+--------------+\n|Pranci|{eye -> brown, hair -> black}|[brown, black]|\n|Tanna |{eye -> blue, hair -> black} |[blue, black] |\n+------+-----------------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Function - 3 --> map_value()\n",
    "from pyspark.sql.functions import map_values\n",
    "df1 = df.withColumn(\"values\",map_values(df.properties))\n",
    "df1.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eab9e085-b9d8-45d0-b715-2d2235f38e66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Row() Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e680a0b-40fb-43cc-98c5-b268c3da3fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pranci 2000\n"
     ]
    }
   ],
   "source": [
    "#Method 1 \n",
    "from pyspark.sql import Row\n",
    "row = Row(\"Pranci\",2000)\n",
    "print(row[0]+\" \"+str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f376e188-c161-4a74-a4d6-45fcf87b973b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanna 2000\n"
     ]
    }
   ],
   "source": [
    "# Method 2 - using named argumnets\n",
    "from pyspark.sql import Row\n",
    "row = Row(name =\"Tanna\",salary = 2000)\n",
    "print(row.name+\" \"+str(row.salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb2dde8e-f392-42ed-8563-699778e45524",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n|  name|salary|\n+------+------+\n|Pranci|  2000|\n| Tanna|  2000|\n+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Method 2 - Multiple row data\n",
    "from pyspark.sql import Row\n",
    "row1 = Row(name =\"Pranci\",salary = 2000)\n",
    "row2 = Row(name =\"Tanna\",salary = 2000)\n",
    "data =[row1,row2]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57151d72-8f86-4740-a339-a9b552ee5aa2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pranci Tanna\n"
     ]
    }
   ],
   "source": [
    "# Method 3 - Row like Class\n",
    "from pyspark.sql import Row\n",
    "Person = Row(\"name\",\"Salary\")\n",
    "p1 = Person(\"Pranci\",\"Agrahari\")\n",
    "p2 = Person(\"Tanna\",\"Gupta\")\n",
    "print(p1.name + \" \" + p2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7886143f-706c-465e-a560-23cfc36297a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n|  name|          prop|\n+------+--------------+\n|Pranci| {Black, Blue}|\n| Tanna|{Black, Brown}|\n+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Method 4 - Nested struct type also using Row()\n",
    "from pyspark.sql import Row\n",
    "data = [Row(name = \"Pranci\",prop = Row(hair = \"Black\",eye = \"Blue\")),\n",
    "        Row(name = \"Tanna\",prop = Row(hair = \"Black\",eye = \"Brown\"))]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e3da6a2-d8d8-407b-a6f7-0b9fe3584edc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bfe131f-5add-486c-989c-0865b4319628",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n|    name|gender|salary|\n+--------+------+------+\n|  Pranci|Female|  2000|\n|Prashant|  Male|  4000|\n+--------+------+------+\n\n+--------+------+------+-------+\n|    name|gender|salary|country|\n+--------+------+------+-------+\n|  Pranci|Female|  2000|  India|\n|Prashant|  Male|  4000|  India|\n+--------+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "data = [(\"Pranci\",\"Female\",2000),(\"Prashant\",\"Male\",4000)]\n",
    "schema = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df1 = df.withColumn(\"country\",lit(\"India\"))\n",
    "df1.show()\n",
    "# lit() is used to add new column with same data in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d50a820-4dce-4f86-b074-845734989b34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|    name|\n+--------+\n|  Pranci|\n|Prashant|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#Access column from dataframe\n",
    "## Method -1\n",
    "df1.select(df1.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30561578-18a5-40d8-9825-50f0cdf130ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|salary|\n+------+\n|  2000|\n|  4000|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Method -2\n",
    "df1.select(df1[\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df0e995d-b11b-4b09-958c-3987d078bcdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|gender|\n+------+\n|Female|\n|  Male|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Method -3\n",
    "from pyspark.sql.functions import col\n",
    "df1.select(col(\"gender\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2d68563-175e-4a30-8550-4dcfaed16578",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark_Functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
